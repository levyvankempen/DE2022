{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3bf0613-d6ec-4cf4-87e5-062fd3bd3a82",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f1d825-84cc-43ac-9fe2-f204d77f0962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.14 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\n",
      "google-cloud-pipeline-components 1.0.25 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install -U google-cloud-storage {USER_FLAG} -q\n",
    "! pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc6a21-604f-4a52-b904-e3bb18a61b2f",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52dad0c4-c173-46b8-bf99-d6e8efc35316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     # Automatically restart kernel after installs\n",
    "#     import IPython\n",
    "\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2207b06-771f-4dbb-a713-90c50745c0ea",
   "metadata": {},
   "source": [
    "Check the versions of the packages you installed. The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b60838-e5a2-41cd-ae93-43925343fba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.14\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0bcff2-3ffb-4e51-b852-511cb10ad0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2.components import importer_node\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afffb0-449b-4669-807a-793f526277fe",
   "metadata": {},
   "source": [
    "#### Project and Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf6aad4-f675-47aa-820b-14daa796d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "project_id = \"de2022-362707\"\n",
    "# The region that this pipeline runs in\n",
    "region = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "pipeline_root_path = \"gs://de_jads_temp_2093373\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bace39-57ba-49ee-bd74-9eaf4093f471",
   "metadata": {},
   "source": [
    "#### Create Pipeline Components\n",
    "\n",
    "We can create a component from Python functions (inline) and from a container. We will first try inline python functions. \n",
    "\n",
    "Step 1: Define the python function\n",
    "\n",
    "Step 2:  Use **kfp.components.create_component_from_func** build the component. This function takes four parameters.\n",
    "\n",
    "**1.func**: The Python function to convert.\n",
    "\n",
    "**2.base_image**: (Optional.) Specify the Docker container image to run this function in. \n",
    "\n",
    "**3.output_component_file**: (Optional.) Writes your component definition to a file. \n",
    "\n",
    "**4.packages_to_install**: (Optional.) A list of versioned Python packages to install before running your function.\n",
    "\n",
    "Another thing we need to consider is passing parameters between components. We can pass simple parameters such as integer, string, tuple, dict, and list by values. To pass the large datasets or complex configurations, we can use files. We can annotate the Python functionâ€™s parameters to indicate input or output files for the component. \n",
    "\n",
    "Refer to  https://www.kubeflow.org/docs/components/pipelines/sdk/python-function-components/ for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457ef88-cd95-4304-b6e0-143b718c44aa",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83bc305f-2456-4c07-b89f-427b0f24eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\",\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    "    output_component_file=\"data_ingestion.yaml\"\n",
    ")\n",
    "def download_data(project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]):\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Downloaing the file from a google bucket \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.download_to_filename(dataset.path + \".csv\")\n",
    "    logging.info('Downloaded Data!')    \n",
    "    logging.info(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3494d2-bea7-415f-9832-fcf1f2c9fe4a",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff87fc3-27f0-4968-a870-7cad067b999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=['google-cloud-storage', 'pandas', 'scikit-learn'],\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    "    output_component_file=\"train_lr_model.yaml\"\n",
    ")\n",
    "def train_lr (features: Input[Dataset], project_id: str, model_repo: str) -> dict:\n",
    "    '''train a LogisticRegression with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "        \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    logging.info('print before loading df')\n",
    "    df = pd.read_csv(features.path+\".csv\")\n",
    "    \n",
    "    logging.info('df loaded')\n",
    "    logging.info(df.columns)        \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.drop('class',axis=1), \n",
    "                                                    df['class'], test_size=0.30, \n",
    "                                                    random_state=101)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x_train,y_train)\n",
    "\n",
    "    metrics = {\"accuracy\": model.score(x_test, y_test)}\n",
    "    logging.info(metrics)\n",
    "   \n",
    "    # Save the model localy\n",
    "    local_file = '/tmp/local_model.pkl'\n",
    "    with open(local_file, 'wb') as file:  \n",
    "        pickle.dump(model, file)\n",
    "    # write out output\n",
    "  \n",
    "    # Save to GCS as model.h5\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('lrmodel.pkl')\n",
    "   # Upload the locally saved model\n",
    "    blob.upload_from_filename(local_file)\n",
    "  \n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911d312-549c-4be7-bef0-e02c9d8cf80f",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acee72f0-007a-4e9c-853a-e6cf66f2a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=['google-cloud-storage', 'pandas', 'scikit-learn'],\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    "    output_component_file=\"train_svc_model.yaml\"\n",
    ")\n",
    "def train_svc (features: Input[Dataset], project_id: str, model_repo: str) -> dict:\n",
    "    '''train a SVM with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "        \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info('print before loading df')\n",
    "    \n",
    "    df = pd.read_csv(features.path+\".csv\") \n",
    "    \n",
    "    logging.info('df loaded')\n",
    "    \n",
    "    logging.info(df.columns)        \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.drop('class',axis=1), \n",
    "                                                    df['class'], test_size=0.30, \n",
    "                                                    random_state=101)\n",
    "    logging.info('split')\n",
    "    \n",
    "    model = SVC()\n",
    "    model.fit(x_train,y_train)\n",
    "\n",
    "    logging.info('model fit')\n",
    "    \n",
    "    metrics = {\"accuracy\": model.score(x_test, y_test)}\n",
    "    logging.info(metrics)\n",
    "   \n",
    "    # Save the model localy\n",
    "    local_file = '/tmp/local_model.pkl'\n",
    "    with open(local_file, 'wb') as file:  \n",
    "        pickle.dump(model, file)\n",
    "    # write out output\n",
    "  \n",
    "    # Save to GCS as model.h5\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('svcmodel.pkl')\n",
    "   # Upload the locally saved model\n",
    "    blob.upload_from_filename(local_file)\n",
    "  \n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f4e9c1-a1d4-412f-8533-8e741fd3f79c",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Prediction-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268815d4-6d66-4fbe-88be-d3407b24c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=['pandas', 'scikit-learn'],\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    "    output_component_file=\"prediction_lr_3.yaml\"\n",
    ")\n",
    "def predict_lr(project_id: str, model_repo: str, features: Input[Dataset]) -> dict:\n",
    "    import pandas as pd\n",
    "    import pickle  \n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.read_csv(features.path+\".csv\")\n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('lrmodel.pkl')\n",
    "    filename = '/tmp/local_model.pkl'\n",
    "    blob.download_to_filename(filename)\n",
    "    \n",
    "    #Loading the model\n",
    "    model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    xNew = df[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    " 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']]\n",
    "    \n",
    "    dfcp = df.copy()   \n",
    "    y_classes = model.predict(xNew)\n",
    "    logging.info(type(y_classes))\n",
    "    logging.info(y_classes)\n",
    "    dfcp['pclass'] = y_classes.tolist()\n",
    "    dic = dfcp.to_dict() \n",
    "    logging.info(len(dic))\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b9f62-14d3-4b97-bd9a-f15b3d47758a",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Prediction SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "590b6be6-b8fe-40b6-ba73-ef1bf07dcb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=['pandas', 'scikit-learn'],\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    "    output_component_file=\"prediction_svc_3.yaml\"\n",
    ")\n",
    "def predict_svc(project_id: str, model_repo: str, features: Input[Dataset]) -> dict:\n",
    "    import pandas as pd\n",
    "    import pickle  \n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.read_csv(features.path+\".csv\")  \n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('svcmodel.pkl')\n",
    "    filename = '/tmp/local_model.pkl'\n",
    "    blob.download_to_filename(filename)\n",
    "    \n",
    "    #Loading the model\n",
    "    model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    xNew = df[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    " 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']]\n",
    "\n",
    "    dfcp = df.copy()   \n",
    "    y_classes = model.predict(xNew)\n",
    "    logging.info(y_classes)\n",
    "    dfcp['pclass'] = y_classes.tolist()\n",
    "    dic = dfcp.to_dict() \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c183e7-f9dd-48d7-b37e-69646bb08203",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Algorithm Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3704a7d-7956-47f2-bdf7-8886519cef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    "    output_component_file=\"compare_model.yaml\"\n",
    ")\n",
    "def compare_model(svc_metrics: dict, lr_metrics: dict) -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(svc_metrics)\n",
    "    logging.info(lr_metrics)\n",
    "    if svc_metrics.get(\"accuracy\") > lr_metrics.get(\"accuracy\"):\n",
    "        return \"SVC\"\n",
    "    else :\n",
    "        return \"LR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166590b3-f788-4e4c-8e31-fb981da56966",
   "metadata": {},
   "source": [
    "#### Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a96b6ae0-234b-4883-ae95-8599689a5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"whitewine-predictor-training-pipeline\",\n",
    "    pipeline_root=pipeline_root_path)\n",
    "def pipeline(project_id: str, data_bucket: str, trainset_filename: str, model_repo: str, testset_filename: str, ):\n",
    "    \n",
    "    \n",
    "    di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=trainset_filename\n",
    "    )\n",
    "\n",
    " \n",
    "    training_svc_job_run_op = train_svc(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=di_op.outputs[\"dataset\"]\n",
    "    )\n",
    "    \n",
    "     \n",
    "    training_lr_job_run_op = train_lr(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=di_op.outputs[\"dataset\"]\n",
    "    )\n",
    "    \n",
    "    pre_di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=testset_filename\n",
    "    ).after(training_svc_job_run_op, training_lr_job_run_op)\n",
    "        \n",
    "        \n",
    "    comp_model__op = compare_model(training_svc_job_run_op.output,\n",
    "                                       training_lr_job_run_op.output).after(training_svc_job_run_op, training_lr_job_run_op)  \n",
    "    \n",
    "    # defining the branching condition\n",
    "    with dsl.Condition(comp_model__op.output==\"SVC\"):\n",
    "        predict_svc_job_run_op = predict_svc(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=pre_di_op.outputs[\"dataset\"]\n",
    "        )\n",
    "    with dsl.Condition(comp_model__op.output==\"LR\"):\n",
    "        predict_lr_job_run_op = predict_lr(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=pre_di_op.outputs[\"dataset\"]\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac278200-c580-4f40-bc8b-1817d3b13c13",
   "metadata": {},
   "source": [
    "#### Compile the pipeline into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8ee4b21-89e6-4f63-845c-b249556ea919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='whitewine_predictor_training_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87025e-08d7-4608-b37d-c929b6eb5a3c",
   "metadata": {},
   "source": [
    "#### Submit the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83b88e89-42cd-4e64-bc4e-8e3eddebccff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/171238576493/locations/us-central1/pipelineJobs/whitewine-predictor-training-pipeline-20221022141904\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/171238576493/locations/us-central1/pipelineJobs/whitewine-predictor-training-pipeline-20221022141904')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/whitewine-predictor-training-pipeline-20221022141904?project=171238576493\n",
      "PipelineJob projects/171238576493/locations/us-central1/pipelineJobs/whitewine-predictor-training-pipeline-20221022141904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/171238576493/locations/us-central1/pipelineJobs/whitewine-predictor-training-pipeline-20221022141904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/171238576493/locations/us-central1/pipelineJobs/whitewine-predictor-training-pipeline-20221022141904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/171238576493/locations/us-central1/pipelineJobs/whitewine-predictor-training-pipeline-20221022141904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/171238576493/locations/us-central1/pipelineJobs/whitewine-predictor-training-pipeline-20221022141904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/171238576493/locations/us-central1/pipelineJobs/whitewine-predictor-training-pipeline-20221022141904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [condition-2].; Job (project_id = de2022-362707, job_id = 8795328861626695680) is failed due to the above error.; Failed to handle the job: {project_number = 171238576493, job_id = 8795328861626695680}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/489509128.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     def submit(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [condition-2].; Job (project_id = de2022-362707, job_id = 8795328861626695680) is failed due to the above error.; Failed to handle the job: {project_number = 171238576493, job_id = 8795328861626695680}\"\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"whitewine-predictor\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"whitewine_predictor_training_pipeline.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id, # makesure to use your project id \n",
    "        'data_bucket': 'data_de2022_2093373',  # makesure to use your data bucket name \n",
    "        'trainset_filename': 'training_set.csv',     # makesure to upload these to your data bucket from DE2022/lab4/data\n",
    "        'testset_filename': 'prediction_set.csv',    # makesure to upload these to your data bucket from DE2022/lab4/data\n",
    "        'model_repo':'model_repo_de2022_2093373' # makesure to use your model bucket name \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5c8c3-ecfe-4f5e-be55-af97ae580124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m84"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
