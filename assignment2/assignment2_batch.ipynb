{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b150b1e1-623a-46bc-9190-b32620a08786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_category_name: string (nullable = true)\n",
      " |-- product_name_lenght: string (nullable = true)\n",
      " |-- product_description_lenght: string (nullable = true)\n",
      " |-- product_photos_qty: string (nullable = true)\n",
      " |-- product_weight_g: string (nullable = true)\n",
      " |-- product_length_cm: string (nullable = true)\n",
      " |-- product_height_cm: string (nullable = true)\n",
      " |-- product_width_cm: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_zip_code_prefix: string (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"GCSExample_ass2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "#  Google Storage File Path\n",
    "gcs_file_paths = ['gs://data_de2022_2093373/olist_customers_dataset.csv', \n",
    "                'gs://data_de2022_2093373/olist_products_dataset.csv', \n",
    "                'gs://data_de2022_2093373/olist_sellers_dataset.csv']\n",
    "\n",
    "# Create data frames\n",
    "customers = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .load(gcs_file_paths[0])\n",
    "customers.printSchema()\n",
    "    \n",
    "products = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .load(gcs_file_paths[1])\n",
    "products.printSchema()\n",
    "    \n",
    "sellers = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .load(gcs_file_paths[2])\n",
    "sellers.printSchema()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d56959c-d2a2-487c-959d-4139e65e00d1",
   "metadata": {},
   "source": [
    "### Do preprocessing here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e0b9a-723d-4ab2-a8b2-2b1149a475be",
   "metadata": {},
   "source": [
    "#### Create new column with regions of Brazil and which states belong to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6085758-e8ee-4082-9de7-079df6b65694",
   "metadata": {},
   "outputs": [],
   "source": [
    "southeast = ['SP', 'RJ', 'ES','MG']\n",
    "northeast = ['MA', 'PI', 'CE', 'RN', 'PE', 'PB', 'SE', 'AL', 'BA']\n",
    "north = ['AM', 'RR', 'AP', 'PA', 'TO', 'RO', 'AC']\n",
    "centerwest = ['MT', 'GO', 'MS' ,'DF' ]\n",
    "south = ['SC', 'RS', 'PR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ace1487-8dd9-48c0-9478-2dca44357633",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers.withColumn(\"customer_zip_code_prefix\",customers.customer_zip_code_prefix.cast('int')) # change to integer\n",
    "customers = customers.withColumn('customer_region', f.when(f.col('customer_state').isin(southeast), f.lit('Southeast'))\n",
    "                                           .when(f.col('customer_state').isin(northeast), f.lit('Northeast'))\n",
    "                                           .when(f.col('customer_state').isin(north), f.lit('North'))\n",
    "                                           .when(f.col('customer_state').isin(centerwest), f.lit('Centerwest'))\n",
    "                                           .when(f.col('customer_state').isin(south), f.lit('South')))\n",
    "customers = customers.withColumnRenamed(\"customer_id\", \"customer_order_id\") # change name to better fit column purpose\n",
    "customers = customers.sort(\"customer_state\") # sort on the state's for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a09f79ed-5207-40df-9f2c-89184399c18e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sellers = sellers.withColumn(\"seller_zip_code_prefix\",sellers.seller_zip_code_prefix.cast('int')) # change to integer\n",
    "sellers = sellers.withColumn('seller_region', f.when(f.col('seller_state').isin(southeast), f.lit('Southeast'))\n",
    "                                           .when(f.col('seller_state').isin(northeast), f.lit('Northeast'))\n",
    "                                           .when(f.col('seller_state').isin(north), f.lit('North'))\n",
    "                                           .when(f.col('seller_state').isin(centerwest), f.lit('Centerwest'))\n",
    "                                           .when(f.col('seller_state').isin(south), f.lit('South')))\n",
    "sellers = sellers.sort('seller_state') # sort on the state's for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaeb7a2a-0734-446a-99f3-96f2fc66587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products.withColumnRenamed('product_name_lenght', 'product_name_length') # rename as there was a typo in the dataset\n",
    "products = products.withColumnRenamed('product_description_lenght', 'product_description_length') # rename as there was a typo in the dataset\n",
    "products = products.withColumn('product_name_length', products.product_name_length.cast('int')) # change to integer\n",
    "products = products.withColumn('product_description_length', products.product_description_length.cast('int')) # change to integer\n",
    "products = products.withColumn('product_photos_qty', products.product_photos_qty.cast('int')) # change to integer\n",
    "products = products.withColumn('product_weight_g', products.product_weight_g.cast('int')) # change to integer\n",
    "products = products.withColumn('product_length_cm', products.product_length_cm.cast('int')) # change to integer\n",
    "products = products.withColumn('product_height_cm', products.product_height_cm.cast('int')) # change to integer\n",
    "products = products.withColumn('product_width_cm', products.product_width_cm.cast('int')) # change to integer\n",
    "products = products.sort('product_category_name') # sort on the product name's for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04e718-3ba2-4447-9f4b-f68368bea1b8",
   "metadata": {},
   "source": [
    "### Saving to BigQuery through bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b616bf72-bb90-4912-baf5-6353104de401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"de_jads_temp_2093373\"  # use your bucket \n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Saving the data to BigQuery\n",
    "customers.write.format('bigquery') \\\n",
    "  .option('table', 'de2022-362707.assignment2.customers') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()\n",
    "\n",
    "sellers.write.format('bigquery') \\\n",
    "  .option('table', 'de2022-362707.assignment2.sellers') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()\n",
    "\n",
    "products.write.format('bigquery') \\\n",
    "  .option('table', 'de2022-362707.assignment2.products') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a9409-1804-40e5-a289-6f1be743e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c17c5273-fd69-4af5-8fba-a87cd81d45ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\documents\\\\YEAR6_JADS_M1\\x01_Data_Engineering\\\\Assignment2\\\\Data\\\\olist_orders_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mYEAR6_JADS_M1\u001b[39;49m\u001b[38;5;130;43;01m\\1\u001b[39;49;00m\u001b[38;5;124;43m_Data_Engineering\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAssignment2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43molist_orders_dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m         lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\documents\\\\YEAR6_JADS_M1\\x01_Data_Engineering\\\\Assignment2\\\\Data\\\\olist_orders_dataset.csv'"
     ]
    }
   ],
   "source": [
    "with open('D:\\documents\\YEAR6_JADS_M1\\1_Data_Engineering\\Assignment2\\Data\\olist_orders_dataset.csv') as f:\n",
    "        lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c73f3-6820-4a0f-ae7f-28cecbe4c5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
