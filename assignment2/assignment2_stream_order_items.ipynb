{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388ef920-66ab-4f41-8a83-b6b4187fd926",
   "metadata": {},
   "source": [
    "### Pyspark code to read from 'order_items' stream, preprocess, and write to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af745f2e-c12d-4a2f-bb0a-6d447b6ceb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, concat, col, lit\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, DateType, IntegerType, FloatType\n",
    "from time import sleep\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"assignment2_stream2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8748fb2d-efb6-49d8-baa1-aa84738d38f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- shipping_limit_date: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- freight_value: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- shipping_limit_date: date (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- freight_value: float (nullable = true)\n",
      " |-- total_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import from_csv\n",
    "\n",
    "# Define schema\n",
    "order_items_dataSchema = StructType(\n",
    "        [StructField(\"order_id\", StringType(), True),\n",
    "         StructField(\"order_item_id\", IntegerType(), True),\n",
    "         StructField(\"product_id\", StringType(), True),\n",
    "         StructField(\"seller_id\", StringType(), True),\n",
    "         StructField(\"shipping_limit_date\", StringType(), True),\n",
    "         StructField(\"price\", FloatType(), True),       \n",
    "         StructField(\"freight_value\", FloatType(), True)\n",
    "         ])\n",
    "\n",
    "# Read the whole dataset as a batch\n",
    "kafkaStream = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "        .option(\"subscribe\", \"order_items\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "df = kafkaStream.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df1 = df.select(from_csv(df.value, order_items_dataSchema.simpleString()))\n",
    "\n",
    "order_items = df1.select(col(\"from_csv(value).*\"))\n",
    "order_items.printSchema()\n",
    "\n",
    "# preprocessing\n",
    "order_items = order_items.withColumn(\"shipping_limit_date\",order_items.shipping_limit_date.cast(DateType())) # change to datetype\n",
    "order_items = order_items.withColumn(\"total_price\", order_items.order_item_id*(order_items.price + order_items.freight_value)) # Add 'total price' column to order_items \n",
    "\n",
    "order_items.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe910406-3e20-4d5c-8426-1134ac6e9428",
   "metadata": {},
   "source": [
    "### Saving to BigQuery as batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2f79773-e9e5-4ffc-ba41-103217a3f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to set the following configuration whenever we need to use GCS.\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"de_jads_temp_2093373\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c0efb7f-81f1-47b1-aed7-18fcc1f25a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoped the streaming query and the spark context\n"
     ]
    }
   ],
   "source": [
    "def my_foreach_batch_function(df, batch_id):\n",
    "   # Saving the data to BigQuery as batch processing sink -see, use write(), save(), etc.\n",
    "    df.write.format('bigquery') \\\n",
    "      .option('table', 'de2022-362707.assignment2.order_items') \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n",
    "\n",
    "# Write to a sink - here, the output is written to a Big Query Table\n",
    "# ProcessingTime trigger with 60-seconds micro-batch interval as the dataset is large and does not get updated within the 60 second timeframe\n",
    "# Using output mode append as only new rows need to be appeneded to BigQuery and no aggregating is done with previous data\n",
    "order_itemsQuery = order_items.writeStream.outputMode(\"append\") \\\n",
    "                        .trigger(processingTime = '60 seconds').foreachBatch(my_foreach_batch_function).start()\n",
    "try:\n",
    "    order_itemsQuery.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    order_itemsQuery.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d401cfd3-560d-418e-954d-49727517dfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
